 Ed: 0, train_loss: 1.68796, acc: 0.10938
 Ed: 640, train_loss: 1.45524, acc: 0.54119
 Ed: 1280, train_loss: 1.42116, acc: 0.51190
 Ed: 1920, train_loss: 1.38675, acc: 0.52268
 Ed: 2560, train_loss: 1.33683, acc: 0.54192
 Ed: 3200, train_loss: 1.31872, acc: 0.54871
 Ed: 3840, train_loss: 1.28414, acc: 0.55943
 Ed: 4480, train_loss: 1.27415, acc: 0.56470
 Ed: 5120, train_loss: 1.25550, acc: 0.56906
 Ed: 5760, train_loss: 1.23702, acc: 0.57658
 Ed: 6400, train_loss: 1.22529, acc: 0.57905
 Ed: 7040, train_loss: 1.21446, acc: 0.58235
 Ed: 7680, train_loss: 1.20106, acc: 0.58600
 Ed: 8320, train_loss: 1.18749, acc: 0.58969
 Ed: 8960, train_loss: 1.18786, acc: 0.58621
 Ed: 9600, train_loss: 1.17648, acc: 0.58961
 Ed: 10240, train_loss: 1.17171, acc: 0.58987
 Ed: 10880, train_loss: 1.16574, acc: 0.59101
 Ed: 11520, train_loss: 1.15939, acc: 0.59194
 Ed: 12160, train_loss: 1.15232, acc: 0.59301
 Ed: 12800, train_loss: 1.14660, acc: 0.59375
 Ed: 13440, train_loss: 1.14208, acc: 0.59516
 Ed: 14080, train_loss: 1.13740, acc: 0.59559
 Ed: 14720, train_loss: 1.13183, acc: 0.59733
 Ed: 15360, train_loss: 1.12461, acc: 0.59881
 Ed: 16000, train_loss: 1.11769, acc: 0.60110
 Ed: 16640, train_loss: 1.11469, acc: 0.60087
 Ed: 17280, train_loss: 1.11020, acc: 0.60090
 Ed: 17920, train_loss: 1.10785, acc: 0.60098
 Ed: 18560, train_loss: 1.10933, acc: 0.59982
 Ed: 19200, train_loss: 1.10591, acc: 0.60112
 Ed: 19840, train_loss: 1.10344, acc: 0.60159
 Ed: 20480, train_loss: 1.10130, acc: 0.60164
 Ed: 21120, train_loss: 1.09856, acc: 0.60140
 Ed: 21760, train_loss: 1.09504, acc: 0.60223
 Ed: 22400, train_loss: 1.08980, acc: 0.60372
 Ed: 23040, train_loss: 1.08851, acc: 0.60349
 Ed: 23680, train_loss: 1.08729, acc: 0.60293
 Ed: 24320, train_loss: 1.08643, acc: 0.60244
 Ed: 24960, train_loss: 1.08423, acc: 0.60302
 Ed: 25600, train_loss: 1.08274, acc: 0.60283
 Ed: 26240, train_loss: 1.07954, acc: 0.60390
 Ed: 26880, train_loss: 1.07674, acc: 0.60455
 Ed: 27520, train_loss: 1.07610, acc: 0.60405
 Ed: 28160, train_loss: 1.07457, acc: 0.60438
 Ed: 28800, train_loss: 1.07207, acc: 0.60487
 Ed: 29440, train_loss: 1.07127, acc: 0.60432
 Ed: 30080, train_loss: 1.06942, acc: 0.60420
 Ed: 30720, train_loss: 1.06676, acc: 0.60473
 Ed: 31360, train_loss: 1.06524, acc: 0.60502
 Ed: 32000, train_loss: 1.06377, acc: 0.60488
 Ed: 32640, train_loss: 1.06188, acc: 0.60519
 Ed: 33280, train_loss: 1.05970, acc: 0.60611
 Ed: 33920, train_loss: 1.05827, acc: 0.60643
 Ed: 34560, train_loss: 1.05599, acc: 0.60730
 Ed: 35200, train_loss: 1.05411, acc: 0.60739
 Ed: 35840, train_loss: 1.05221, acc: 0.60776
 Ed: 36480, train_loss: 1.05123, acc: 0.60757
 Ed: 37120, train_loss: 1.04928, acc: 0.60800
 Ed: 37760, train_loss: 1.04766, acc: 0.60837
 Ed: 38400, train_loss: 1.04634, acc: 0.60904
 Ed: 39040, train_loss: 1.04582, acc: 0.60925
 Ed: 39680, train_loss: 1.04358, acc: 0.60993
accuracy: 
0.615
macrof: 
0.3649
 Ed: 0, train_loss: 0.81754, acc: 0.75000
 Ed: 640, train_loss: 0.92574, acc: 0.65767
 Ed: 1280, train_loss: 0.92746, acc: 0.64062
 Ed: 1920, train_loss: 0.92650, acc: 0.64617
 Ed: 2560, train_loss: 0.92300, acc: 0.64482
 Ed: 3200, train_loss: 0.92199, acc: 0.64338
 Ed: 3840, train_loss: 0.92901, acc: 0.64062
 Ed: 4480, train_loss: 0.92115, acc: 0.64349
 Ed: 5120, train_loss: 0.91522, acc: 0.64757
 Ed: 5760, train_loss: 0.91073, acc: 0.64852
 Ed: 6400, train_loss: 0.90833, acc: 0.64851
 Ed: 7040, train_loss: 0.91272, acc: 0.64682
 Ed: 7680, train_loss: 0.91148, acc: 0.64605
 Ed: 8320, train_loss: 0.91181, acc: 0.64528
 Ed: 8960, train_loss: 0.90995, acc: 0.64661
 Ed: 9600, train_loss: 0.90672, acc: 0.64714
 Ed: 10240, train_loss: 0.90562, acc: 0.64829
 Ed: 10880, train_loss: 0.90660, acc: 0.64766
 Ed: 11520, train_loss: 0.90725, acc: 0.64744
 Ed: 12160, train_loss: 0.90553, acc: 0.64864
 Ed: 12800, train_loss: 0.90693, acc: 0.64692
 Ed: 13440, train_loss: 0.90602, acc: 0.64699
 Ed: 14080, train_loss: 0.90468, acc: 0.64777
 Ed: 14720, train_loss: 0.90371, acc: 0.64881
 Ed: 15360, train_loss: 0.90374, acc: 0.64983
 Ed: 16000, train_loss: 0.90235, acc: 0.65096
 Ed: 16640, train_loss: 0.89962, acc: 0.65170
 Ed: 17280, train_loss: 0.89705, acc: 0.65342
 Ed: 17920, train_loss: 0.89647, acc: 0.65375
 Ed: 18560, train_loss: 0.89707, acc: 0.65297
 Ed: 19200, train_loss: 0.89775, acc: 0.65246
 Ed: 19840, train_loss: 0.89750, acc: 0.65263
 Ed: 20480, train_loss: 0.89442, acc: 0.65425
 Ed: 21120, train_loss: 0.89305, acc: 0.65512
 Ed: 21760, train_loss: 0.89321, acc: 0.65465
 Ed: 22400, train_loss: 0.89334, acc: 0.65465
 Ed: 23040, train_loss: 0.89434, acc: 0.65474
 Ed: 23680, train_loss: 0.89490, acc: 0.65473
 Ed: 24320, train_loss: 0.89483, acc: 0.65469
 Ed: 24960, train_loss: 0.89369, acc: 0.65477
 Ed: 25600, train_loss: 0.89413, acc: 0.65457
 Ed: 26240, train_loss: 0.89353, acc: 0.65507
 Ed: 26880, train_loss: 0.89361, acc: 0.65454
 Ed: 27520, train_loss: 0.89423, acc: 0.65426
 Ed: 28160, train_loss: 0.89490, acc: 0.65412
 Ed: 28800, train_loss: 0.89492, acc: 0.65410
 Ed: 29440, train_loss: 0.89484, acc: 0.65388
 Ed: 30080, train_loss: 0.89451, acc: 0.65413
 Ed: 30720, train_loss: 0.89488, acc: 0.65404
 Ed: 31360, train_loss: 0.89526, acc: 0.65367
 Ed: 32000, train_loss: 0.89523, acc: 0.65316
 Ed: 32640, train_loss: 0.89350, acc: 0.65377
 Ed: 33280, train_loss: 0.89302, acc: 0.65394
 Ed: 33920, train_loss: 0.89264, acc: 0.65387
 Ed: 34560, train_loss: 0.89303, acc: 0.65356
 Ed: 35200, train_loss: 0.89297, acc: 0.65313
 Ed: 35840, train_loss: 0.89323, acc: 0.65333
 Ed: 36480, train_loss: 0.89249, acc: 0.65316
 Ed: 37120, train_loss: 0.89248, acc: 0.65302
 Ed: 37760, train_loss: 0.89138, acc: 0.65369
 Ed: 38400, train_loss: 0.89149, acc: 0.65344
 Ed: 39040, train_loss: 0.89222, acc: 0.65308
 Ed: 39680, train_loss: 0.89122, acc: 0.65331
accuracy: 
0.6136
macrof: 
0.4685
 Ed: 0, train_loss: 0.83182, acc: 0.68750
 Ed: 640, train_loss: 0.79565, acc: 0.68750
 Ed: 1280, train_loss: 0.81075, acc: 0.67411
 Ed: 1920, train_loss: 0.80418, acc: 0.67994
 Ed: 2560, train_loss: 0.80245, acc: 0.67683
 Ed: 3200, train_loss: 0.78626, acc: 0.68474
 Ed: 3840, train_loss: 0.79565, acc: 0.68212
 Ed: 4480, train_loss: 0.79867, acc: 0.68112
 Ed: 5120, train_loss: 0.79799, acc: 0.68152
 Ed: 5760, train_loss: 0.79639, acc: 0.68252
 Ed: 6400, train_loss: 0.79483, acc: 0.68379
 Ed: 7040, train_loss: 0.79006, acc: 0.68651
 Ed: 7680, train_loss: 0.79170, acc: 0.68608
 Ed: 8320, train_loss: 0.78998, acc: 0.68655
 Ed: 8960, train_loss: 0.78861, acc: 0.68661
 Ed: 9600, train_loss: 0.78992, acc: 0.68709
 Ed: 10240, train_loss: 0.79196, acc: 0.68527
 Ed: 10880, train_loss: 0.79252, acc: 0.68549
 Ed: 11520, train_loss: 0.79191, acc: 0.68646
 Ed: 12160, train_loss: 0.79406, acc: 0.68480
 Ed: 12800, train_loss: 0.79011, acc: 0.68633
 Ed: 13440, train_loss: 0.78675, acc: 0.68757
 Ed: 14080, train_loss: 0.79018, acc: 0.68594
 Ed: 14720, train_loss: 0.79035, acc: 0.68662
 Ed: 15360, train_loss: 0.79293, acc: 0.68607
 Ed: 16000, train_loss: 0.79435, acc: 0.68520
 Ed: 16640, train_loss: 0.79555, acc: 0.68463
 Ed: 17280, train_loss: 0.79781, acc: 0.68398
 Ed: 17920, train_loss: 0.80039, acc: 0.68266
 Ed: 18560, train_loss: 0.80300, acc: 0.68181
 Ed: 19200, train_loss: 0.80307, acc: 0.68205
 Ed: 19840, train_loss: 0.80321, acc: 0.68212
 Ed: 20480, train_loss: 0.80258, acc: 0.68215
 Ed: 21120, train_loss: 0.80439, acc: 0.68108
 Ed: 21760, train_loss: 0.80610, acc: 0.68040
 Ed: 22400, train_loss: 0.80605, acc: 0.68064
 Ed: 23040, train_loss: 0.80551, acc: 0.68032
 Ed: 23680, train_loss: 0.80625, acc: 0.68030
 Ed: 24320, train_loss: 0.80695, acc: 0.68086
 Ed: 24960, train_loss: 0.80692, acc: 0.68087
 Ed: 25600, train_loss: 0.80624, acc: 0.68134
 Ed: 26240, train_loss: 0.80618, acc: 0.68153
 Ed: 26880, train_loss: 0.80580, acc: 0.68145
 Ed: 27520, train_loss: 0.80665, acc: 0.68105
 Ed: 28160, train_loss: 0.80557, acc: 0.68162
 Ed: 28800, train_loss: 0.80576, acc: 0.68130
 Ed: 29440, train_loss: 0.80651, acc: 0.68109
 Ed: 30080, train_loss: 0.80706, acc: 0.68070
 Ed: 30720, train_loss: 0.80650, acc: 0.68087
 Ed: 31360, train_loss: 0.80745, acc: 0.68053
 Ed: 32000, train_loss: 0.80740, acc: 0.68083
 Ed: 32640, train_loss: 0.80720, acc: 0.68093
 Ed: 33280, train_loss: 0.80737, acc: 0.68021
 Ed: 33920, train_loss: 0.80863, acc: 0.67950
 Ed: 34560, train_loss: 0.80935, acc: 0.67933
 Ed: 35200, train_loss: 0.81049, acc: 0.67885
 Ed: 35840, train_loss: 0.81058, acc: 0.67848
 Ed: 36480, train_loss: 0.81182, acc: 0.67800
 Ed: 37120, train_loss: 0.81240, acc: 0.67825
 Ed: 37760, train_loss: 0.81398, acc: 0.67796
 Ed: 38400, train_loss: 0.81394, acc: 0.67806
 Ed: 39040, train_loss: 0.81456, acc: 0.67814
 Ed: 39680, train_loss: 0.81487, acc: 0.67809
accuracy: 
0.6334
macrof: 
0.437
